{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means using Spark"
      ],
      "metadata": {
        "id": "wEKchs_F3xH0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCDOl5oM3e1r",
        "outputId": "b99468e0-2f65-4077-dfcc-f4baa712491b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 51 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 64.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=71a25df03de46c779af44045d6ef3352bcf09176788a9e4ae1f4b26f5250d0f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 22 not upgraded.\n",
            "Need to get 36.6 MB of archives.\n",
            "After this operation, 143 MB of additional disk space will be used.\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 123941 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u342-b07-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u342-b07-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark #This is usually for local usage or as a client to connect to a cluster instead of setting up a cluster itself.\n",
        "!pip install -U -q PyDrive #PyDrive is a wrapper library of google-api-python-client that simplifies many common Google Drive API tasks.\n",
        "!apt install openjdk-8-jdk-headless -qq #software capable of working on a device without a graphical user interface. Such software receives inputs and provides output through other interfaces like network or serial port and is common on servers and embedded devices.\n",
        "import os #The os module is a part of the standard library, or stdlib, within Python 3. This means that it comes with your Python installation, but you still must import it\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" #The path is the most important environment variable of the Java environment which is used to locate the JDK packages that are used to convert the java source code into the machine-readable binary format"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd #Pandas is mainly used for data analysis and associated manipulation of tabular data in DataFrames. \n",
        "import numpy as np #NumPy is a Python library used for working with arrays\n",
        "import matplotlib.pyplot as plt #Data Vizualization\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark # PySpark is the Python API for Apache Spark, an open source, distributed computing framework and set of libraries for real-time, large-scale data processing.\n",
        "from pyspark.sql import * #Spark SQL is a Spark module for structured data processing. It provides a programming abstraction called DataFrames and can also act as distributed SQL query engine.\n",
        "from pyspark.sql.types import * #PySpark SQL Types class is a base class of all data types in PuSpark which defined in a package pyspark.sql.types.DataType and they are used to create DataFrame with a specific type\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf #A SparkContext represents the connection to a Spark cluster, and can be used to create RDD and broadcast variables on that cluster\n",
        "#ublic class SparkConf extends java.lang.Object implements scala.Cloneable, Logging. Configuration for a Spark application. "
      ],
      "metadata": {
        "id": "g26ZicKv4hr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a session\n",
        "conf = SparkConf().set(\"spark.ui.port\",\"4050\") #working with the port number\n",
        "\n",
        "#Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "#This method first checks whether there is a valid global default SparkSession, and if yes, return that one. If no valid global default SparkSession exists, the method creates a new SparkSession and assigns the newly created SparkSession as the global default."
      ],
      "metadata": {
        "id": "lFM5Knp97S-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "GJuic8ny8BMj",
        "outputId": "718c6948-e1d6-41b1-ffbc-14a5874117b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f8c49cbfc90>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://5ba79c0e84b6:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip #ngrok is the fastest way to put anything on the internet with a single command.\n",
        "!unzip ngrok-stable-linux-amd64.zip #ngrok is a cross-platform application that enables developers to expose a local development server to the Internet with minimal effort. \n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnmiCpDz9Oq-",
        "outputId": "c39a91bf-1e61-4bd3-e7be-c0bb1e4400fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-19 18:17:54--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 18.205.222.128, 52.202.168.65, 54.161.241.46, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|18.205.222.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  14.0MB/s    in 0.9s    \n",
            "\n",
            "2022-10-19 18:17:55 (14.0 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13832437/13832437]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "IndexError: list index out of range\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "breast_cancer = load_breast_cancer()"
      ],
      "metadata": {
        "id": "0LAJ4tME9wzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd_df = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "df = spark.createDataFrame(pd_df)\n",
        "\n",
        "def set_df_columns_nullable(spark, df, column_list, nullable=False):\n",
        "  for struct_field in df.schema:\n",
        "    if struct_field.name in column_list:\n",
        "      struct_field.nullable = nullable\n",
        "  df_mod = spark.createDataFrame(df.rdd,df.schema)\n",
        "  return df_mod\n",
        "\n",
        "df = set_df_columns_nullable(spark, df, df.columns)\n",
        "df = df.withColumn('features', array(df.columns))\n",
        "vectors = df.rdd.map(lambda row: Vectors.dense(row.features))\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1aM_StO-A2I",
        "outputId": "a9288599-7638-4b6f-c617-5326afb640e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- mean radius: double (nullable = false)\n",
            " |-- mean texture: double (nullable = false)\n",
            " |-- mean perimeter: double (nullable = false)\n",
            " |-- mean area: double (nullable = false)\n",
            " |-- mean smoothness: double (nullable = false)\n",
            " |-- mean compactness: double (nullable = false)\n",
            " |-- mean concavity: double (nullable = false)\n",
            " |-- mean concave points: double (nullable = false)\n",
            " |-- mean symmetry: double (nullable = false)\n",
            " |-- mean fractal dimension: double (nullable = false)\n",
            " |-- radius error: double (nullable = false)\n",
            " |-- texture error: double (nullable = false)\n",
            " |-- perimeter error: double (nullable = false)\n",
            " |-- area error: double (nullable = false)\n",
            " |-- smoothness error: double (nullable = false)\n",
            " |-- compactness error: double (nullable = false)\n",
            " |-- concavity error: double (nullable = false)\n",
            " |-- concave points error: double (nullable = false)\n",
            " |-- symmetry error: double (nullable = false)\n",
            " |-- fractal dimension error: double (nullable = false)\n",
            " |-- worst radius: double (nullable = false)\n",
            " |-- worst texture: double (nullable = false)\n",
            " |-- worst perimeter: double (nullable = false)\n",
            " |-- worst area: double (nullable = false)\n",
            " |-- worst smoothness: double (nullable = false)\n",
            " |-- worst compactness: double (nullable = false)\n",
            " |-- worst concavity: double (nullable = false)\n",
            " |-- worst concave points: double (nullable = false)\n",
            " |-- worst symmetry: double (nullable = false)\n",
            " |-- worst fractal dimension: double (nullable = false)\n",
            " |-- features: array (nullable = false)\n",
            " |    |-- element: double (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "features = spark.createDataFrame(vectors.map(Row),[\"features\"])\n",
        "labels = pd.Series(breast_cancer.target)"
      ],
      "metadata": {
        "id": "7nFWFlKr_zNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "# Train a k-means model\n",
        "kmeans = KMeans().setK(2).setSeed(1)\n",
        "model = kmeans.fit(features)\n",
        "\n",
        "# Making predictions\n",
        "predictions = model.transform(features)\n",
        "\n",
        "# Evaluate clustering by computing Silhouette score\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette score with squared eucledian distance: \" + str(silhouette))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpfb9edJCYpm",
        "outputId": "7355f503-38fb-41be-9b6a-e399003c5ddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette score with squared eucledian distance: 0.8342904262826145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2-ReQKWHKdCf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}